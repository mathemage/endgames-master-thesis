\chapter{Setting the Scene for Perfect Information}
\label{ch:perf-intro}
\epigraphLong{
  In order to improve your game, you must study the endgame before everything else.
  For whereas the endings can be studied and mastered by~themselves, the middle game and opening must be studied in relation to the end game.
}{José Raúl Capablanca}
For many games, solving their late stages (so-called \emph{endgames}) can be done in~a~dynamic online way.
In other words, we are often able to postpone the computation of~the endgame strategy until the endgame itself is reached in~the play.

On the other hand, endgames can be also pre-computed (often up to a~perfect play) and cached for later.
Such a~``memoization'' approach is notably fruitful in~popular games such as Chess.

\section{Extensive Form for Perfect-Information}
\label{sec:extensive-form-perf-info}
\epigraph{Trees that are slow to grow bear the best fruit.}
{Molière}
Games with perfect information can be naturally represented with a~\emph{game tree}:
\begin{figure}[H]
  \centering
  \scriptsize
  \def\svgwidth{.9\textwidth}
  \input{../img/game_tree_nim_5.pdf_tex}
  \def\captionTitle{Game tree of~(1,2)-Nim with 1~heap of 5~stones}
  \caption[\captionTitle]{\captionTitle{} (rendered by \codeText{Gambit} using the included example \codeText{nim.efg})}
  \label{fig:game-tree-nim-5}
\end{figure}
\noindent
The representation with a~(directed) tree (rather than with a~pay-off matrix) is called an extensive form.

Formally, an~\emph{\acrfull{efg}} for a~perfect-information game (\cite[p.~200]{Osborne1994course}) consists of:
\begin{itemize}
  \item a~finite set of~\emph{players}~$P$,

  \item a~finite set $H$ of~all possible \emph{histories} (i.e., paths from the root to vertices of~the game tree), where
    \begin{itemize}
      \item each history consists of~individual \emph{actions} (i.e., tree edges),
      \item $\emptyset \in H$ (the root of~the game tree),
      \item relation~$h \sqsubseteq h'$ denotes that history~$h$ is a~prefix of $h'$,
      \item if $h' \in H$ and $h \sqsubseteq h'$, then $h \in H$,
      \item set $Z \subseteq H$ is the set of \emph{terminal histories}, i.e., histories that are not prefixes of~any other histories.
    \end{itemize}

  \item the set of available actions $A(h) = \braces{a: (h, a) \in H}$ for every (non-terminal) node $h \in H \setminus Z$ (i.e., edges to~children in the game tree),

  \item a~function $p\colon P \to H \setminus Z$, which assigns an~\emph{acting player} $p(h)$ to each non-terminal history~$h$.

  \item a~\emph{utility function} $u_i\colon Z \goto \R$.
\end{itemize}
We will also need the notion of~a~strategy, the expected utility and a~best response.
\begin{itemize}
  \item A~(behavior) \emph{strategy}~$\sigma_i$ of player~$i$ defines a~probability distribution over actions $A(h)$ at~every node~$h \in H$ with $p(h) = i$ (i.e., everywhere where player~$i$ acts).
    The set~$\Sigma_i$ contains all possible strategies of player~$i$, and $\sigma_{[S \leftarrow \sigma^*]}$ means the combined strategy of~$\sigma^*$ (within~$S$) and $\sigma$ (elsewhere).

  Additionally, a~strategy can be \emph{pure} (just one action is strictly chosen everywhere) or \emph{mixed} (a~probability distribution over pure strategies).

  \item A~\emph{strategy profile} $\sigma$ is a~vector of~players' strategies:
    $\sigma = (\sigma_1, \sigma_2, \ldots, \sigma_ {\abs{P}})$.
    The set of~all such possible strategy profiles is denoted by~$\Sigma$.
    Hence, it is the Cartesian product $\Sigma = \prod_{i \in P} \Sigma_i$.

    By~$\sigma_{-i}$ we denote the strategy profile of~$i$'s opponents:
    \[ \sigma_{-i} = (\sigma_1, \sigma_2, \ldots \sigma_{i-1}, \sigma_{i+1}, \ldots \sigma_ {\abs{P}}) \]

  \item We use the symbol $\pi^\sigma$ (resp. $\pi_i^\sigma$) to evaluate the overall (resp. player~$i$'s) probability corresponding to a~strategy profile~$\sigma$.
    The probability $\pi^\sigma(h)$ of~reaching node~$h$ can be decomposed to each player's contribution as
    \[ \pi ^\sigma(h) = \prod _{i \in P} \pi _i ^\sigma (h) \]

    Furthermore, the probability $\pi _{-i} ^\sigma (h)$ (shortly $\pi _{-i} (h)$) is the product of~all players' contribution, except for $i$'s one:
    \[ \pi _{-i} ^\sigma(h) = \prod _{j \ne i} \pi _j ^\sigma (h) \]
    
  \item Given a~strategy profile $\sigma$, the \emph{expected utility}~$u_i (\sigma)$ for player~$i$
    is the sum of~utilities in~the leaves (weighted by the probabilities of~reaching them):
    \[ u_i (\sigma) = \sum _{z \in Z} \pi^\sigma\!(z) \,u_i(z)\]

  \item Player~$i$'s \emph{best response}~$BR _i (\sigma _{-i})$ (briefly $BR _i (\sigma)$) is a~strategy $\sigma _i \in \Sigma _i$ maximizing his expected utility against other players:
    \[ u_i (\sigma) = \max _{\sigma'_i \in \Sigma_i} u_i ((\sigma'_i, \sigma_{-i})) \]
    Provided a~fixed $\sigma_{-i}$, one technique to find some best response is to recursively traverse the game tree and pick each node's most valuable action (with the maximum utility).
\note{
  There may be several best responses due to a~multiplicity of~actions with maximum value.
  Let us assume $BR _i (\sigma _{-i})$ denotes any of~them:
  this simplification is not overly harmful, since all best responses lead to the same (maximum) value of~the total utility.
}

  \item A~\emph{\acrfull{ne}} is a~strategy profile where players are playing best responses against each other.
    Formally, profile $\sigma$ is an~\acrshort{ne} if
    \[ \forall i \in P\colon u_i (\sigma) = \max _{\sigma'_i \in \Sigma_i} u_i ((\sigma'_i, \sigma_{-i})) \]
    (i.e., no player has any incentive to deviate from his strategy).

  \item For a~given profile~$\sigma$, its exploitability
    \[
      \epsilon_\sigma = \frac{u_1(CBR(\sigma_2), \sigma_2) + u_2(\sigma_1, CBR(\sigma_1))} {2}
    \]
    expresses how much $\sigma$ loses to a~worst-case opponent, averaged over both players.
    An~\acrshort{ne} has an~exploitability of~$0$, because it is unexploitable.
\end{itemize}

Finally, two common properties of~games are:
\begin{itemize}
  \item \emph{two-player} property
  \item \emph{zero-sum} property:
    For any profile $\sigma \in \Sigma$, players' utilities cancel out
    \[ \sum _{i \in P} u _i (\sigma) = 0 \]
    Examples of~zero-sum games include games such as Matching Pennies, \acrshort{rps}, Chess, Go and Poker.
\end{itemize}
In particular, two-player zero-sum games capture the~antagonistic behavior:
what one player gains, the other one loses.
Typical examples are \acrshort{rps}, matching pennies, Chess, Go, heads-up poker etc.

\section{Subgames}
\epigraphLong{
  At that time I did not suspect that it [``An Oligopoly Model with Demand Inertia'' (\cite{Selten1968oligopoly})] often would be quoted, almost exclusively for the definition of subgame perfectness.
}{Reinhard Selten}
In~the perfect-information setting, a~\emph{subgame} is a~game corresponding to a~subtree of~the game tree.
Specifically, any node in~the tree induces its own subgame:
\begin{figure}[H]
  \centering
  \scriptsize
  \def\svgwidth{.7\textwidth}
  \input{../img/extended-form_subgame.pdf_tex}
  \def\captionTitle{Subgame induced by node~$2_2$}
  \caption[\captionTitle]{\captionTitle{}}
  \label{fig:ext-form-subgame}
\end{figure}

\note{
  For the upcoming proofs, we define the notation for \emph{the~restriction of the expected utility} on a~subgame rooted in node~$h\in H$:
  \[ u_i^h(\sigma) = \sum _{z \in Z,\; h \sqsubseteq z} \pi^\sigma\!(h,z) \,u_i(z) \]
  where $\pi^\sigma(h,z)$ is the probability of~reaching $z$ from node~$h$ under $\sigma$.
}

If we fix opponents' strategies, it is only beneficial to re-solve subgames:%
\footnote{This property also holds for imperfect-information subgames.}
\begin{thm}[subgame re-solving and utility]
  \label{thm:perf-info-subgames-utility}
  The re-computed sub-strategies can be ``placed back'' into the full-game strategy, without decreasing the total expected utility.
\end{thm}
\begin{proof}
  Let $\sigma$ be any~full-game strategy and $\sigma_S$ be its restriction to a~subgame~$S$ rooted at node~$r$.
  Assume we found a~better (or even optimal) strategy $\sigma^*$ for the subgame~$S$.
  In other words, the arising expected utility of~any player~$i$ is better within $S$:
  \begin{equation}
    \label{eq:opt-val-of-subgame}
    u_i^r(\sigma^*) \ge u_i^r(\sigma_S)
  \end{equation}
  The new re-combined strategy $\sigma' = \sigma_{[S \leftarrow \sigma^*]}$ cannot decrease $i$'s utility:
  \begin{align*}
    u_i(\sigma')
    %
    &= \sum _{z \in Z} \pi^{\sigma'}\!(z) \,u_i(z)
    %
    = \sum _{z \in Z \cap S} \pi^{\sigma'}\!(z) \,u_i(z)
    + \sum _{z \in Z \setminus S} \pi^{\sigma'}\!(z) \,u_i(z) \\
    %
    &= \left[ \pi^{\sigma'}\!(r)\sum _{z \in Z \cap S} \pi^{\sigma^*}\!(r, z) \,u_i(z) \right]
    + \sum _{z \in Z \setminus S} \pi^{\sigma'}\!(z) \,u_i(z) \\
    %
    &= \pi^{\sigma}\!(r) \,u_i^r(\sigma^*)
    + \sum _{z \in Z \setminus S} \pi^{\sigma}\!(z) \,u_i(z)
    %
    \stackrel{\mathclap{\mathrm{(\ref{eq:opt-val-of-subgame})}}} {\ge}
    \pi^{\sigma}\!(r) \,u_i^r(\sigma_S)
    + \sum _{z \in Z \setminus S} \pi^{\sigma}\!(z) \,u_i(z) \\
    %
    &= \sum _{z \in Z \cap S} \pi^{\sigma}\!(z) \,u_i(z)
    + \sum _{z \in Z \setminus S} \pi^{\sigma}\!(z) \,u_i(z)
    %
    = u_i(\sigma)
  \end{align*}
  Therefore, combining the new $\sigma^*$ with the full-game $\sigma$ does not decrease the total expected utility.
\end{proof}

The perfect-information setting offers one additional advantage over the imperfect information:
no opponent can exploit the newly computed sub-strategy by adjusting his play (either in~the trunk or in~the subgame).
\begin{thm}[unexploitable subgame re-solving]
  \label{thm:perf-info-subgames-unexploitability}
  Assume we are given a~two-person zero-sum game, its subgame~$S$ rooted at node~$r$ and a~fixed player~$i$.
  Let $\sigma_i$ be the~original strategy, $\sigma_i^*$ the re-computed strategy that only differs within $S$, let $\sigma_{-i} = BR(\sigma _{i})$ and $\sigma_{-i}^* = BR(\sigma^*_{i})$ be opponent's corresponding best responses, and let $\sigma = (\sigma_i, \sigma_{-i})$ and $\sigma^* = (\sigma_i^*, \sigma_{-i}^*)$ be the resulting profiles.

  If $u_i^r (\sigma) \le u_i^r (\sigma^*)$ (player~$i$'s worst-case utility does not decrease in~the subgame), then at~every node $h \in H \setminus S$ we also have $u_i^h (\sigma) \le u_i^h (\sigma^*)$ (player~$i$'s worst-case utilities outside the subgame do not decrease).
\end{thm}
\begin{proof}
  By definition, all best responses produce equal expected utilities, because they all maximize opponent's value.
  We can therefore do calculations of~utilities with respect to any~arbitrary best response, e.g., the standard best response retrieved by~the \emph{backward induction}\footnotemark.
  \footnotetext{This method recursively selects the action with the maximum utility, or any of~them in~the case of~multiplicity.}

  Let $T_h$ be the subtree rooted at~$h$.
  If $T_h$ does not contain $S$ (i.e., $h \not\sqsubseteq r$), then the best responses $\sigma_{-i}$ and $\sigma^*_{-i}$ are identical within $T_h$, which trivially implies $u_i^h (\sigma) = u_i^h (\sigma^*)$.
  On~the other hand, if $T_h$ does contain $S$, we will prove the inequality by~mathematical induction on the height of~$T_h$.

  For the base case, the minimum-height tree containing $S$ is the subgame tree $S \equiv T_r$ itself, where the inequality holds by assumptions.
  For the inductive step, let node~$h$ has $k$ children $h_1, \dots h_k$, and without loss of~generality, $T_{h_1}$ contains $S$.
  By~the induction hypothesis, $u_i^{h_1} (\sigma) \le u_i^{h_1} (\sigma^*)$.
  Because the subtrees of~the remaining children don't contain $S$ (i.e., $h_j \not\sqsubseteq r$ for $j = 2, \dots, k$), $\sigma$ and $\sigma^*$ are equal there.
  Hence, $u_i^{h_j} (\sigma) = u_i^{h_j} (\sigma^*)$ for $j = 2, \dots, k$.
  Therefore, we have
  \begin{equation}
    \label{eq:children-inc-util}
    u_i^{h_j}(\sigma) \le u_i^{h_j}(\sigma^*)
  \end{equation}
  If $i$ is the acting player of~$h$ (i.e., $p(h) = i$), then by~expressing utilities as weighted sums of~children's utilities we get
  \begin{align*}
    u_i^h(\sigma)
    %
    &= \sum _{j = 1, \dots, k} \pi^\sigma\!(h,h_j) \,u_i^{h_j}(\sigma)
    %
    = \sum _{j = 1, \dots, k} \pi^{\sigma^*}\!(h,h_j) \,u_i^{h_j}(\sigma) \\
    %
    &\stackrel{\mathclap{\mathrm{(\ref{eq:children-inc-util})}}} {\le}
    \sum _{j = 1, \dots, k} \pi^{\sigma^*}\!(h,h_j) \,u_i^{h_j}(\sigma^*)
    %
    = u_i^h(\sigma^*)
  \end{align*}
  On the other hand, if the opponent is the one who is acting (i.e., $p(h) \ne i$), he aims to choose the action with $i$'s minimum utility (due to the zero-sum property):
  \begin{align*}
    u_i^h(\sigma)
    %
    = \min _{j = 1, \dots, k} \,u_i^{h_j}(\sigma)
    %
    \stackrel{\mathclap{\mathrm{(\ref{eq:children-inc-util})}}} {\le}
    \min _{j = 1, \dots, k} \,u_i^{h_j}(\sigma^*)
    %
    = u_i^h(\sigma^*)
  \end{align*}
  In either cases $u_i^h(\sigma) \le u_i^h(\sigma^*)$.
\end{proof}

\begin{cor}
  If a~player employs a~re-computed subgame strategy, the opponent has no way to~adjust his strategy, in order to increase his own overall utility.
\end{cor}

This means that we can deal with subgames of~perfect-information games separately, either by~pre-computation or by~dynamic re-solving.
E.g., this approach has met successful in Checkers with (otherwise) intractable $5 \times 10^{20}$ states:
Checkers has been solved both in~terms of the game's value and an~optimal \acrshort{ne} strategy (\cite{Schaeffer2007checkers}).

Moreover, we may freely combine the newly found strategies with the original ones:
Theorem~\ref{thm:perf-info-subgames-utility} guarantees they won't be inferior, and Theorem~\ref{thm:perf-info-subgames-unexploitability} guarantees their unexploitability by the opponent.

\note{
  The situation in imperfect-information setting is different:
  in~fact, Claim~\ref{claim:rps-subgame} proves that a~re-solved subgame strategy can be actually exploited.
}

\section{Working Examples}
\epigraph{
  Few things are harder to~put up with than the~annoyance of a~good example.
}{Mark Twain}
Subgame solutions are particularly used in~perfect-information games with an~extensive form such as Chess or Go, where the endgame technique has been used for~long time as one way to~defeat the colossal size of~the game tree.
In these specific domains, endgame solving has~additional significance, as it improves the playing quality of~agents as well.

In Chapter~\ref{ch:Chess}, we will see the power of~subgame pre-computation with the example of~Chess:
solutions to~endings are stored in so-called \emph{endgame tablebase}.
They are used in~real world to~aid professional Chess players, either in~proving their guaranteed victory or in~analysing past games.
Moreover, since tablebases are mathematically proven to be~optimal, they provide a~glimpse into the world of~``perfect Chess'' played by ``all-knowing super-players''.

In contrast, Chapter~\ref{ch:goCGT} demonstrates how the in-play approach to~endgames is beneficial in~the game of~Go.
Once reaching the late stage, the board is partitioned into distinct parts.
The corresponding (and provably independent) subgames are re-solved ``on the fly'', just to be afterwards combined using the \emph{\acrlong{cgt}}.

Finally, Chapter~\ref{ch:AlphaGo} reviews the modern approach to~computer Go employed by~Google DeepMind:
the Go program \emph{AlphaGo} combines \emph{Monte Carlo tree search} with \emph{neural networks} to~simulate every play position, as if it were an~endgame.
This in particular means that several moves into the future are simulated and the corresponding part of~the game tree is unrolled and expanded.
The rest of~the tree is discarded, effectively leaving only the relevant subgame for the oncoming play.
