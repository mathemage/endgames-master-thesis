\chapter{Setting the Scene for Perfect Information}
\epigraph{
  In order to improve your game, you must study the endgame before everything else.
  For whereas the endings can be studied and mastered by~themselves, the middle game and opening must be studied in relation to the end game.
}{José Raúl Capablanca}

For many games, solving their late stages (so-called \emph{endgames}) can be done in~a~dynamic online way.
In other words, we are often able to postpone the computation of~the endgame strategy until the endgame itself is reached in~the play.

On the other hand, endgames can be also pre-computed (often up to a~perfect play) and cached for later.
Such a~``memoization'' approach is notably fruitful in~popular games such as Chess.

\section{Extensive Form for Perfect-Information}
\label{sec:extensive-form-perf-info}
\epigraph{Trees that are slow to grow bear the best fruit.}
{Molière}
Games with perfect information can be naturally represented with a~\emph{game tree}:
\begin{figure}[H]
  \centering
  \scriptsize
  \def\svgwidth{.9\textwidth}
  \input{../img/game_tree_nim_5.pdf_tex}
  \def\captionTitle{Game tree of~(1,2)-Nim with 1~heap of 5~stones}
  \caption[\captionTitle]{\captionTitle{}\\ (rendered by \codeText{Gambit} using the included example \codeText{nim.efg})}
  \label{fig:game-tree-nim-5}
\end{figure}
\noindent
The representation with a~(directed) tree (rather than with a~pay-off matrix) is called an extensive form.

Formally, an~\emph{extensive-form game} (\cite[p.~200]{Osborne1994course}) for a~perfect-information game consists of:
\begin{itemize}
  \item a~finite set of~\emph{players}~$P$,

  \item a~finite set $H$ of~all possible \emph{histories} (i.e. paths from the root to vertices of~the game tree), where
    \begin{itemize}
      \item each history consists of~individual \emph{actions} (i.e. tree edges),
      \item $\emptyset \in H$ (the root of~the game tree),
      \item relation~$h \sqsubseteq h'$ denotes that history~$h$ is a~prefix of $h'$,
      \item if $h' \in H$ and $h \sqsubseteq h'$, then $h \in H$,
      \item set $Z \subseteq H$ is the set of \emph{terminal histories}, i.e. histories that are not prefixes of~any other histories.
    \end{itemize}

  \item the set of available actions $A(h) = \braces{a: (h, a) \in H}$ for every (non-terminal) node $h \in H \setminus Z$ (i.e. edges to~children in the game tree),

  \item a~function $p\colon P \to H \setminus Z$, which assigns an~\emph{acting player} $p(h)$ to each non-terminal history~$h$.

  \item a~\emph{utility function} $u_i\colon Z \goto \R$.
\end{itemize}
We will also need the notion of~a~strategy, the expected utility and a~best response.
\begin{itemize}
  \item A~(behavior) \emph{strategy}~$\sigma_i$ of player~$i$ defines a~probability distribution over actions $A(h)$ wherever he acts (i.e. at~every node~$h \in H$ with $p(h) = i$).
    The set~$\Sigma_i$ contains all possible strategies for player~$i$.

  Additionally, a~strategy can be \emph{pure} (just one action is strictly chosen everywhere) or \emph{mixed} (a~probability distribution over pure strategies).

  \item A~\emph{strategy profile} $\sigma$ is a~vector of~players' strategies:
    $\sigma = (\sigma_1, \sigma_2, \ldots, \sigma_ {\abs{P}})$.
    The set of~all such possible strategy profiles is denoted by~$\Sigma$.
    Hence, it is the Cartesian product $\Sigma = \prod_{i \in P} \Sigma_i$.

  \item We use the symbol $\pi^\sigma$ (resp. $\pi_i^\sigma$) to evaluate the overall (resp. player~$i$'s) probability corresponding to a~strategy profile~$\sigma$.
    The probability $\pi^\sigma(h)$ of~reaching node~$h$ can be decomposed to each player's contribution as $\pi ^\sigma(h) = \prod _{i \in P} \pi _i ^\sigma (h)$.

  \item Given a~strategy profile $\sigma$, the \emph{expected utility}~$u_i (\sigma)$ for player~$i$
    is the sum of~utilities in~the leaves (weighted by the probabilities of~reaching them):
    \[ u_i (\sigma) = \sum _{z \in Z} \pi^\sigma\!(z) \,u_i(z)\]

    For our purposes, let us define a~restriction of the expected utility on a~subgame rooted in node~$h\in H$:
    \[ u_i^h(\sigma) = \sum _{z \in Z,\; h \sqsubseteq z} \pi^\sigma\!(h,z) \,u_i(z) \]
    where $\pi^\sigma(h,z)$ is the probability of~reaching $z$ from node~$h$.

  \item Player~$i$'s \emph{best response}~$BR _i (\sigma _{-i})$ (briefly $BR _i (\sigma)$) is a~strategy $\sigma _i \in \Sigma _i$ maximizing his expected utility against other players:
    \[ u_i (\sigma) = \max _{\sigma'_i \in \Sigma_i} u_i ((\sigma'_i, \sigma_{-i})) \]
    Provided a~fixed $\sigma_{-i}$, one technique to find some best response is to recursively traverse the game tree and pick each node's most valuable action (with the maximum utility).
\end{itemize}
\note{
  There may be several best responses due to a~multiplicity of~actions with maximum value.
  Let us assume $BR _i (\sigma _{-i})$ denotes any of~them:
  this simplification is not overly harmful, since all best responses lead to the same (maximum) value of~the total utility.
}

\section{Subgames}
\epigraph{
  At that time I did not suspect that it [``An Oligopoly Model with Demand Inertia'' (\cite{Selten1968oligopoly})] often would be quoted, almost exclusively for the definition of subgame perfectness.
}{Reinhard Justus Reginald Selten}
In~the perfect-information setting, a~\emph{subgame} is a~game corresponding to a~subtree of~the game tree.
Specifically, any node in~the tree induces its own subgame:
\begin{figure}[H]
  \centering
  \scriptsize
  \def\svgwidth{.7\textwidth}
  \input{../img/extended-form_subgame.pdf_tex}
  \def\captionTitle{Subgame induced by node~$2_2$}
  \caption[\captionTitle]{\captionTitle{}\\ (\href{https://en.wikipedia.org/wiki/Subgame_perfect_equilibrium}{https://en.wikipedia.org/wiki/Subgame\_perfect\_equilibrium})}
  \label{fig:ext-form-subgame}
\end{figure}

One cannot emphasize enough the ideal conditions for~subgames in~the perfect-information setting:

\begin{thm}
  \label{thm:perf-info-subgames}
  All subgames can be re-computed independently.
  In particular, these re-solved sub-strategies can be ``re-substituted'' back into the full-game strategy, without decreasing the total expected utility.
\end{thm}

\begin{proof}
  Let $\sigma$ be any~full-game strategy and $\sigma_S$ be its restriction to a~subgame~$S$ rooted at node~$r$.
  Assume we found a~better (or even optimal) strategy $\sigma^*$ for the subgame~$S$.
  For any player~$i$, the arising expected utility $u^*_i := u_i(\sigma^*)$ is player~$i$'s optimal value within $S$.
  Hence, his utility can only improve:
  \begin{equation}
    u^*_i \ge u_i(\sigma_S)
    \label{eq:opt-val-of-subgame}
  \end{equation}
  The new re-combined strategy $\sigma'$ (i.e. $\sigma^*$ within~$S$, and the original $\sigma$ elsewhere) cannot decrease $i$'s utility:
  \begin{align*}
    u_i(\sigma')
    %
    &= \sum _{z \in Z} \pi^{\sigma'}\!(z) \,u_i(z)
    %
    = \sum _{z \in Z \cap S} \pi^{\sigma'}\!(z) \,u_i(z)
    + \sum _{z \in Z \setminus S} \pi^{\sigma'}\!(z) \,u_i(z) \\
    %
    &= \left[ \pi^{\sigma'}\!(r)\sum _{z \in Z \cap S} \pi^{\sigma^*}\!(r, z) \,u_i(z) \right]
    + \sum _{z \in Z \setminus S} \pi^{\sigma'}\!(z) \,u_i(z) \\
    %
    &= \pi^{\sigma}\!(r) \,u_i^*
    + \sum _{z \in Z \setminus S} \pi^{\sigma}\!(z) \,u_i(z)
    %
    \stackrel{\text{(\ref{eq:opt-val-of-subgame})}}{\ge} \pi^{\sigma}\!(r) \,u_i(\sigma_S)
    + \sum _{z \in Z \setminus S} \pi^{\sigma}\!(z) \,u_i(z) \\
    %
    &= \sum _{z \in Z \cap S} \pi^{\sigma}\!(z) \,u_i(z)
    + \sum _{z \in Z \setminus S} \pi^{\sigma}\!(z) \,u_i(z)
    %
    = u_i(\sigma)
  \end{align*}
  Therefore, combining the new $\sigma^*$ with the full-game $\sigma$ does not decrease the expected utility.
\end{proof}

This means that we can deal with subgames of~perfect-information games separately (either by~pre-computing or by~dynamic in-play re-solving).
Moreover, we may freely combine the newly found strategies with the original ones, since Theorem~\ref{thm:perf-info-subgames} guarantees they won't be inferior.

\section{Working Examples}
\epigraph{
  Few things are harder to~put up with than the~annoyance of a~good example.
}{Mark Twain}
Subgame solutions are particularly used in~perfect-information games with an~extensive form such as Chess or Go, where the endgame technique has been used for~long time as one way to~defeat the colossal size of~the game tree.
In these specific domains, endgame solving has~additional significance, as it improves the playing quality of~agents as well.

In Chapter~\ref{ch:Chess}, we will see the power of~subgame pre-computation with the example of~Chess:
solutions to~endings are stored in so-called \emph{endgame tablebase}.
They are used in~real world to~aid professional Chess players, either in~proving their guaranteed victory or in~analysing past games.
Moreover, since tablebases are mathematically proven to be~optimal, they provide a~glimpse into the world of~``perfect Chess'' played by ``all-knowing super-players''.

In contrast, Chapter~\ref{ch:goCGT} demonstrates how the in-play approach to~endgames is beneficial in~the game of~Go.
Once reaching the late stage, the board is partitioned into distinct parts.
The corresponding (and provably independent) subgames are re-solved ``on the fly'', just to be afterwards combined using the \emph{combinatorial game theory}.

Finally, Chapter~\ref{ch:AlphaGo} reviews the modern approach to~computer Go employed by~Google DeepMind:
the Go program \emph{AlphaGo} combines \emph{Monte Carlo tree search} with \emph{neural networks} to~simulate every play position, as if it were an~endgame.
This in particular means that several moves into the future are simulated and the corresponding part of~the game tree is unrolled and expanded.
The rest of~the tree is discarded, effectively leaving only the relevant subgame for the oncoming play.
