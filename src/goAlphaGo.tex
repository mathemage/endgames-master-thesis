\chapter{Go Endgames Using Neural Networks}
\label{ch:AlphaGo}
\epigraphLong{
  Creativity involves breaking out of established patterns in order to look at things in a different way.
}{Edward de Bono}
As of~today, over 20 years have passed since \Mueller's work on~computer Go (Chapter~\ref{ch:goCGT}).
\emph{DeepMind}, a~London-based AI start-up recently acquired by Google, has developed the strongest computer Go program so far:
\emph{AlphaGo}.

\note{
  This chapter is based on~the article ``Mastering the game of Go with deep neural networks and tree search''~(\cite{Silver2016mastering}), which documents the internals of~AlphaGo.

  For more details, also consult my presentations on AlphaGo given at:
\begin{itemize}
  \item Spring School of Combinatorics 2016 (Charles University) \\
    \href{http://www.slideshare.net/KarelHa1/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search-presentation}
    {\tt http://www.slideshare.net/KarelHa1/mastering-the\\-game-of-go-with-deep-neural-networks-and-tree\\-search-presentation}

  \item Optimization Seminar (Charles University) \\
    \href{http://www.slideshare.net/KarelHa1/alphago-mastering-the-game-of-go-with-deep-neural-networks-and-tree-search}
    {\tt http://www.slideshare.net/KarelHa1/alphago\\-mastering-the-game-of-go-with-deep-neural\\-networks-and-tree-search}

  \item Distributed Computing group (ETH \Zurich) \\
    \href{http://www.slideshare.net/KarelHa1/alphago-for-disco-group-of-eth-zurich}
    {\tt http://www.slideshare.net/KarelHa1/alphago\\-for-disco-group-of-eth-zurich}
\end{itemize}
}

\section{Game-Tree Search}
\label{sec:game-tree-search}
\epigraphLong{
  There are more possible positions in~Go than there are atoms in~the universe.
  That makes Go a~googol [$10^{100}$] times more complex than Chess.
}{\href{https://deepmind.com/alpha-go.html}{Google DeepMind}}
Optimal value~$v^*(s)$ determines the~outcome of~the game from every board position $s$ under perfect play by~all players.
This value can be computed by~recursively traversing the~search tree containing approximately $b^d$ possible sequences of moves, where $b$ is the breadth of~game (number of legal moves per position) and $d$ is its depth (game length).

In~the case of~Chess, these are $b \approx 35$ and $d \approx 80$, whereas Go has $b \approx 250$ and $d \approx 150$.
This amounts to a~terrifying overnumerousness: there are more Go positions than atoms in the observable Universe.
Therefore, exhaustive search is deemed intractable.

How to handle the size~of the game tree?
\begin{itemize}
  \item For the breadth, we train \emph{a~neural network to~select} moves.
  \item For the depth, we train \emph{a~neural network to~evaluate} the current position.
  \item For the tree traverse, we use \emph{Monte Carlo tree search} method.
\end{itemize}

\textbf{\acrfull{mcts}} is a~Monte Carlo heuristic version of~the classical tree search.
However, instead of traversing the entire game tree, the \acrshort{mcts} selects the~most promising moves, expanding the search tree based on~random sampling.

In~each iteration, the game is played-out to~the very end, by~choosing moves at~random.
The final outcome of~each playout is then used to~accordingly weigh the nodes in~the game tree.
Thus, better nodes are more likely to be chosen in future playouts.

\begin{figure}[H]
  \centering
  \includegraphics[width=.6\textwidth]{../img/MCTS.png}
  \caption{The scheme of~\acrshort{mcts}}
  \label{fig:MCTS}
\end{figure}

\section{Neural networks}
\epigraph{
  Your brain does not manufacture thoughts.
  Your thoughts shape neural networks.
}{Deepak Chopra}
Inspired by biological neural networks, an~\acrfull{ann} is a~network of~interconnected nodes that make up a~model (see Figure~\ref{fig:shallow-neural-network}).
\begin{figure}[H]
  \centering
  \includegraphics[height=.2\textheight]{../img/colored_neural_network.png}
  \caption{A~shallow neural network with 3 layers}
  \label{fig:shallow-neural-network}
\end{figure}

We can think of~\acrshort{ann}s as~statistical learning models that are used to approximate functions with a~large number of~inputs.
Neural networks are typically used when the amount of~inputs is far too large for~standard machine learning approaches.

A~\textbf{\acrfull{dnn}} is a~term for any neural network with many hidden layers (but most commonly we use it for \acrlong{cnn}s).
It can model complex non-linear relationships in~domains such as speech, images, videos, board positions in~Go, etc.

A~\textbf{\acrfull{cnn}} is a~neural network suitable for high-dimensional inputs (e.g., many pixels in an~image).
\acrshort{cnn}s are frequently used in~computer vision, e.g., for identifying objects in an~image, face detection in~photos, etc.
They are resistant to expectable transformations of~input, such as changes in~illumination or translations of~objects in~a~picture.

\section{Training Pipeline of~Neural Networks}
\epigraphLong{
  Pattern recognition and association make up the core of~our thought.
  These activities involve millions of~operations carried out in~parallel, outside the field of~our consciousness.
  If AI appeared to hit a~brick wall after a~few quick victories, it did so owing to~its inability to~emulate these processes.
}{Daniel Crevier}
AlphaGo employs two kinds of deep \acrshort{cnn}s---a~\emph{policy network} (for move selection) and a~\emph{value network} (for board evaluation):

\vskip 2em

\begin{figure}[H]
  \centering
  \includegraphics[width=.55\textwidth]{../img/policy_and_value_network.png}
  \captionWithCite{Comparison between policy and value network}{Silver2016mastering}
  \label{fig:policy_vs_value_nets}
\end{figure}
\clearpage

\noindent The following figure outlines the entire training process of~AlphaGo's networks.

\begin{figure}[H]
  \centering
  \includegraphics[width=.7\textwidth]{../img/neural_nets_pipeline.png}
  \captionWithCite{Training the neural networks of~AlphaGo: the~pipeline~and~the~architecture}{Silver2016mastering}
  \label{fig:neural_nets_pipeline}
\end{figure}

\textbf{Rollout policy} $p_\pi$ is a~\acrshort{cnn} rapidly sampling actions during a~\emph{rollout} (a~fast-forward simulation from a~position to the end of a~game).
It predicts expert human moves less accurately but much faster than $p_\sigma$ (below).
The output is a~probability distribution over all moves.

\textbf{Policy network} is a~move-selecting \acrshort{cnn}.
It addresses the problem of the game-tree breadth.
There are two flavors of~these networks:
\begin{itemize}
  \item \textbf{SL policy network} $p_\sigma$ is trained by \underline{s}upervised \underline{l}earning to predict expert human moves.
  \item \textbf{RL policy network} $p_\rho$ is trained by \underline{r}einforcement \underline{l}earning to win in the~games of~self-play.
\end{itemize}

\textbf{Value network} $v_\theta$ is a~\acrshort{cnn} evaluating board positions, so as to address the problem of the game-tree depth.
It is trained by regression to predict the outcome in~positions of~the self-played games.

\section{Main Algorithm of~AlphaGo}
\epigraph{
  Computers are good at~following instructions, but not at~reading your mind.
}{Donald Knuth}
Finally, the neural networks are combined with \acrshort{mcts} into the main algorithm.

During the play, AlphaGo simulates up to 100 possible continuations per each move by selecting the most promising actions and following them.
This way, it descends the game-tree down to a~depth given by a~parameter.
At that point, leaf nodes are evaluated in two ways:
\begin{enumerate}[(1)]
  \item using the dedicated value network $v_\theta$,
  \item simulating the self-play until the terminal positions, using the fast rollout policy $p_\pi$.
\end{enumerate}
The two are mixed into the final value of~the leaf node.

Once all simulations for a~single round are finished, all new values are back-propagated to the root, thus updating necessary variables on the way up.

For more details, consult (\cite{Silver2016mastering}) or my mentioned presentations.

\note{
  Main algorithm demonstrates the connection to \emph{our theme of~endgames}:
  the simulations may be viewed as ``solving endgames''.
  In particular, the $p_\pi$ rollouts somehow remind of~exhaustive search for the game value during the late stage of~the play.
  In this sense, the approaches of~AlphaGo and endgame computation bear a~striking similarity.

  On the other hand, AlphaGo performs the same simulation algorithm during the whole game:
  from an~empty board until the final move.
  Therefore, it would be imprecise to talk about ``endgame''.
}

\section{Playing Strength}
\epigraphLong{
  I know AlphaGo is a~computer, but if no one told me, maybe I would think the player was a~little strange, but a~very strong player, a~real person.
}{Fan Hui}
In order to assess the playing strength of~AlphaGo, DeepMind has organized an~internal tournament against other Go programs.%
\footnote{\emph{CrazyStone} and \emph{Zen} are the strongest commercial programs, whereas \emph{Pachi} and \emph{Fuego} are strongest among the open-source ones.}
Figure~\ref{fig:Go-tournament} displays the outcome of the tournament:
\begin{figure}[H]
  \centering
  \includegraphics[width=.45\textwidth]{../img/results_of_tournament.png}
  \captionWithCite{Tournament with other programs and Fan Hui}{Silver2016mastering}
  \label{fig:Go-tournament}
\end{figure}
The 95\%-confidence intervals were used.
Programs were allowed approximately 5~seconds of~computational time per move, and some of~them (with faded tops of~bars) also later played with four \emph{handicap stones} (i.e., free extra moves at~the beginning of~each game) against all others.

The Elo scale was used to measure the playing strength:
a~230-point gap equals to a~79\%-probability of~winning, as well corresponding to about one \emph{amateur dan} rank difference on~KGS Go Server\footnotemark.
The ranks achieved by~programs on~KGS are shown on~the right side.
\footnotetext{\href{https://www.gokgs.com/}{https://www.gokgs.com/}}

Finally, AlphaGo played two spectacular duels against professional human players:
\begin{description}
  \item [Fan Hui]~
    \begin{itemize}
      \item professional 2\textsuperscript{nd} dan
      \item European Go Champion in 2013, 2014 and 2015
      \item European Professional Go Champion in 2016 
    \end{itemize}
    $\rightarrow$ \textbf{AlphaGo won 5:0} in a~formal match in~October 2015, becoming \emph{the first program to~ever beat a~professional Go player in an~even game}.%
    \footnote{\href{https://deepmind.com/alpha-go.html}{https://deepmind.com/alpha-go.html}}

  \item [Lee Sedol ``The Strong Stone'']~
    \begin{itemize}
      \item professional 9\textsuperscript{th} dan 
      \item the $2\textsuperscript{nd}$ in international titles
      \item the $5\textsuperscript{th}$ youngest (12 years 4 months) to become a~professional Go player in~South Korean history
      \item the top Go player in the world over the past decade
      \item Legendary Lee Sedol would win 97 out of~100 games against Fan Hui.
    \end{itemize}
    $\rightarrow$ \textbf{AlphaGo won 4:1} in March 2016.
    Each game was won by~resignation.

    The winner of~the match was awarded the prize of~\$1,000,000.
    Google DeepMind donated this money to~charities, including~UNICEF, and various Go organisations.

    Lee Sedol received \$170,000 (i.e., \$150,000 for participation in all games and extra \$20,000 for every victory).
\end{description}

\epigraph{
  I have never been so much celebrated and praised for winning one single game.
}{Lee Sedol}

\note{
  Following from this, computers seem to have reached the ``superhuman'' level of~expertise and they now appear to be superior to humans in~the game of~Go.
}
