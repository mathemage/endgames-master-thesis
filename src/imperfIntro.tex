\chapter{Setting the Scene for Imperfect Information}
\label{ch:imperf-intro}
\epigraphLong{
  Intelligence is a~game of~imperfect information.
  We can guess our opponent's moves, but we can't be sure until the game is over.
}{Khalid Muhammad}
So far we were dealing with \emph{perfect-information} games:
neither chance nor hidden information were involved in Chess or Go.
Any player can theoretically (with unlimited computational resources and/or infinite amount of~time) look ahead, investigate all possible continuations, pick the best one and play perfectly.
Of~course in~case of~Chess and Go, an~exhaustive search may easily take longer than the age of~the Universe.
Nevertheless, there are no mathematical limitations preventing such analysis.

\emph{Imperfect-information} games, on the other hand, need to also include private information, e.g., opponent's cards in~Poker.
We cannot (and should not) play differently in~equivalent states, whose indistinguishability is caused by private information of~our opponent.
Such missing knowledge is captured in~the concept of~an~\emph{information set}.

\section{Extensive Form for Imperfect-Information}
\label{sec:extensive-form-imperf-info}
\epigraph{
  Education is only a~ladder to~gather fruit from the tree of~knowledge, not the fruit itself.
}{Albert Einstein}
Recall extensive forms for perfect-information games from Section~\ref{sec:extensive-form-perf-info}.
An~\emph{extensive form} (for an~imperfect-information game) is the usual extensive form with additional:
\begin{itemize}
  \item \textbf{c}hance player $c$ (e.~g. a~dice, the card dealer, the nature etc.).
    The set of~players is thus $P \cup \braces{c}$ and the probability corresponding to a~strategy profile~$\sigma$ includes the chance:
    \[\pi ^\sigma(h) = \prod _{i \in P \cup {c}} \pi _i ^\sigma (h)\]

  \item function $f_c$ determining the probability distribution over actions~$A(h)$ for every chance node~$h$ (i.e., $p(h) = c$).

  \item partition $\I_i$ of nodes $\braces{h \in H: p(h) = i}$, which is called the \emph{information partition} of player~$i$.
    Its element $I \in \I_i$ is an~\emph{information set} of player~$i$ and $I(h) \in \I_i$ (with $p(h) = i$) denotes the information set containing $h$.

    An information set represents grouping of histories that are indistinguishable from $i$'s point of view.
    In the game of poker, for example, this might be because of  opponents' hidden cards.
\end{itemize}
\noindent
\begin{figure}[H]
  \centering
  \scriptsize
  \def\svgwidth{.7\textwidth}
  \input{../img/strategic-form-tree.pdf_tex}
  \def\captionTitle{An~imperfect-information game tree with information sets}
  \caption[\captionTitle]{\captionTitle{} (\cite[p.~67]{AGT07})}
  \label{fig:strategic-form-tree}
\end{figure}
\noindent
There are further notions related to \acrshort{efg}s:
\begin{itemize}
  \item A~(behavior) \emph{strategy}~$\sigma_i$ of player~$i$ gives a~probability distribution over $A(I)$ at every $I \in \I_i$, and $\pi ^\sigma (I, a)$ is the probability of action $a$ at the information set~$I$.
    Again, $\Sigma_i$ denotes the set of all possible strategies for player~$i$.

  \item $\sigma | _{I \goto a}$ denotes the strategy identical to $\sigma$ with the only one exception:
    the action~$a$ is always played at the information set~$I$.

  \item A~\emph{\acrfull{ne}} is (in the context of \acrshort{efg}s) a~strategy profile $\sigma$ such that no player~$i \in P$ has any incentive to deviate from his strategy.
    In other words, all players are playing best responses against each other:
    \[ \forall i \in P\colon u_i (\sigma) = \max _{\sigma'_i \in \Sigma_i} u_i ((\sigma'_i, \sigma_{-i})) \]

  \item The \emph{counterfactual value} $v _i ^\sigma (I)$ is the expected utility provided that the information set $I$ is reached and all players play according to strategy $\sigma$ with exception of player~$i$, who plays to reach $I$:
    \[ v _i ^\sigma (I) = \sum\limits _{h \in I, \; h' \in Z}
      \frac
      {\pi _{-i} ^\sigma(h) \pi ^\sigma(h,h') u_i(h')}
      {\pi _{-i} ^\sigma (I)} \]

  \item A~\emph{\acrlong{cbr}} $CBR _i (\sigma _{-i})$ (briefly $CBR _i (\sigma)$ or $CBR(\sigma_{-i})$) of player~$i$ is a~strategy maximizing the counterfactual value at each information set $I \in \I _i$:
    \[ \pi ^\sigma (I, a) \geq 0
      \; \Longleftrightarrow \;
      v _i ^\sigma (I, a) = \max _{a' \in A(I)} v _i ^\sigma (I, a') \]

    Note that $CBR _i (\sigma)$ is always a best response $BR _i (\sigma)$, but the reverse implication does not need to hold:
    a~best response $\sigma$ can select an~arbitrary action in an~unreachable information set $I$ (the one where $\pi ^\sigma (I) = 0$).
    Such best responses are in general not \acrlong{cbr}s.

  \item For the sake of notation's simplicity, we will define a~\emph{\acrfull{cbv}} as~the counterfactual value for the strategy, where player $i$ plays according $CBR _i (\sigma _{-i})$ rather than the original $\sigma$.
    Formally, it is
    \[ CBV _i ^\sigma (I) = v _i ^{(\sigma _{-i}, CBR _i (\sigma _{-i} ))} (I) \]

  \item An~\acrshort{efg} with (non-singleton) information sets can have a~\emph{perfect recall}:
    any two states from the same information set~$I \in \I _i$ share the same history of~past actions and same passed information sets of~$i$.

    Games without this property have a~\emph{imperfect recall}:
    at~some stage of~the game a~player forgets what happened so far, either some actions taken or some information sets reached.
\end{itemize}

\section{Sequence Form}
\epigraph{
  Error is ever the sequence of~haste.
}{Duke of Wellington}
\vskip -2em
\note{
  This section is based on~\cite[Section~3.10]{AGT07}.
}
One possible way (\cite[pp.~73--74]{AGT07}) to~solve \acrshort{efg}s is by solving a~corresponding \acrfull{lp} called the \emph{strategic form}.
However, the number of~pure strategies (over which players are mixing) is generally exponential, as we need to combine all action choices across all information sets (see Figure~\ref{fig:strategic-form}).
The size of~the \emph{strategic-form} \acrshort{lp} increases rapidly, deeming the problem intractable to~solve.
\begin{figure}[H]
  \centering
  \tiny
  \def\svgwidth{.5\textwidth}
  \input{../img/strategic-form-tree.pdf_tex}
  \includegraphics[width=.4\textwidth]{../img/strategic-form.png}
  \caption[The strategic-form pay-off matrices]{Left: An~\acrshort{efg}. Right: The corresponding strategic-form pay-off matrices. (\cite[p.~67]{AGT07})}
  \label{fig:strategic-form}
\end{figure}

A~possible solution is to~use the \emph{sequence form} (\cite[pp.~70--73]{AGT07}).
This form of~\acrshort{lp} has variables just for \emph{sequences of~actions}.
Only sequences from~the same root-to-leaf paths are then combined:
\begin{figure}[H]
  \centering
  \tiny
  \def\svgwidth{.5\textwidth}
  \input{../img/strategic-form-tree.pdf_tex}
  \includegraphics[width=.4\textwidth]{../img/sequence-form.png}
  \caption[The sequence-form pay-off matrices]{Left: A~game with sequences~$S_1 = \{\emptyset, L, R, LS, LT\}$ and $S_2 = \{\emptyset, l, r \}$.
    Right: The corresponding sequence-form pay-off matrices. (\cite[p.~67]{AGT07})}
  \label{fig:sequence-form}
\end{figure}
\noindent
\begin{itemize}
  \item \emph{Sequence}~$s_i(I)$ describes player~$i$'s history of~actions, i.e., edges passed along the path from the root to information set~$I\in I_i$.

    Set~$S_i$ denotes the set of~all $i$'s valid sequences:
    every non-empty sequence $s\in S_i$ is uniquely determined by its last action $a\in A_{I}$ at~an~information set~$I \in \I_i$.
    We can characterize
    \[
      S_i = \{\emptyset\} \cup \{ s_i(I) \cdot a \mid I\in\I_i,\, a\in A(I) \}
    \]
    which leads to the linear size $\lv S_i \rv = 1 + \sum_{I\in\I_i} \lv A(I) \rv$.

  \item The \emph{realization probability} (of~a~sequence~$s \in S_i$ under a~strategy~$\sigma_i \in \Sigma_i$) is $\pi^{\sigma_i} [s] = \prod_{a \in s} \pi^{\sigma_i}(a)$.

  \item Recall that a~mixed strategy~$\mu_1$ is a~probability distribution over (all) pure strategies of player~$1$.
    A~\emph{realization plan}\footnotemark{} $x\colon S_1 \to \R$ expresses the probability of~following sequence~$s$ under a~mixed strategy~$\mu_1$:
    \footnotetext{For player~$2$, a~realization plan $y\colon S_2 \to \R$ would be defined similarly.}
    \[
      x(s) = \sum _{\textrm{pure strategy } \sigma} \mu_1(\sigma)\; \pi^{\sigma}[s]
    \]
    If $\mu_1$ is a~behavior strategy and sequence~$s$ ends in a~state~$h$, we directly get $x(s) = \pi^{\mu_1}_1(h)$.
\end{itemize}

\section{Solving Games with Linear Programming}
\label{sec:solving-games-lp}
\epigraphLong{
  If you optimize everything, you will always be unhappy.
}{Donald Knuth}
\vskip -2em
\note{
  This section is based on~\cite[Section~3.11]{AGT07}.
}
Realization plans play the role of~variables in sequence-form \acrshort{lp} formulations.
We think of realization plans as vectors
$x := (x_s) _{s\in S_1} \in \mathbb{R} ^{\lv S_1 \rv}$
and
$y := (y_s) _{s\in S_2} \in \mathbb{R} ^{\lv S_2 \rv}$.
Not all vectors $x \ge \vect{0}$, $y \ge \vect{0}$ can be realization plans, though.
We need to add two more types of~\acrshort{lp} constraints.

The first kind of~necessary conditions states that the probability of~a~sequence can be decomposed into probabilities of~its immediate continuations:
\begin{align}
  \label{eq:seq-cond-children}
  \begin{split}
    x_{s_1(I)} &= \sum _{a \in A(I)}    x_{s_1(I \cdot a)}, \quad I\in\I_1, \\
    y_{s_2(I')} &= \sum _{a' \in A(I')} y_{s_2(I' \cdot a')}, \quad I'\in\I_2.
  \end{split}
\end{align}
The second kind of~conditions ensures that the probabilities in~realization plans sum to $1$:
\begin{equation}
  \label{eq:seq-cond-init}
  x_{\emptyset} = 1, \quad y_{\emptyset} = 1
\end{equation}
Interestingly, the conditions (\ref{eq:seq-cond-children}), (\ref{eq:seq-cond-init}) are also sufficient, giving a~full characterization for realization plans~(\cite[Proposition~3.10, p.~71]{AGT07}).

Now we re-formulate (\ref{eq:seq-cond-children}) and (\ref{eq:seq-cond-init}) using \emph{sequence constraint matrices}:
\begin{equation}
  \label{eq:seq-constraints}
  Ex = e, \ x \ge \vect{0}
  \quad \textrm{and} \quad
  Fy = f, \ y \ge \vect{0}.
\end{equation}
The first player gets a~sequence constraint matrix $E \in \{-1, 0, 1\} ^{(1 + \lv \I_1 \rv) \times \lv S_1 \rv}$, where columns are indexed by sequences from $s\in S_1$ and rows by equations of~(\ref{eq:seq-cond-children}) and (\ref{eq:seq-cond-init}).
Specifically, the first row of $Ex = e$ has form
\[
  x _{\emptyset} = 1,
\]
and the row corresponding to an~information set~$I \in \mathcal{I}_1$ looks like
\[
    x _{s_1 (I)} - \sum _{a \in A(I)} x _{s_1(I \cdot a)} = 0.
\]
The \emph{sequence constraint vector}~$e$ on the right-hand side is thus $(1, 0, \dots, 0) ^\top$.
Matrix~$E$ is sparse as it has only $\Onot(\lv \I_1 \rv + \sum_{I\in\I_i} \lv A(I) \rv)$ non-zero elements.
For $F \in \{-1, 0, 1\} ^{(1 + \lv \I_2 \rv) \times \lv S_2 \rv}$ and $f = (1, 0, \dots, 0) ^\top$, constraints $Fy = f$ are analogous.

The \emph{sequence-form payoff matrix}~$A \in \mathbb{R} ^{\lv S_1 \rv \times \lv S_2 \rv}$ of~player~$1$ is described for every sequence $s \in S_1$ and $s' \in S_2$ by
\[
  A _{s,s'} = \sum _{\substack{z \in Z\\ s_1(z) = s,\; s_2(z) = s'}} \pi_c (z) \; u_1(z),
\]
where $\pi_c (z)$ is the contribution of~the chance to the probability, along the path to leaf~$z$.
The sequence-form payoff matrix of~player~$2$ is $-A$ from the zero-sum property.
Because the probability of~reaching any leaf~$z$ can be decomposed as
$
\pi^\sigma(z) =
\pi^{\sigma_1}[s_1(z)] \cdot
\pi^{\sigma_2}[s_2(z)] \cdot
\pi_c(z)
$,
we can write player~$1$'s utility as
\[
  u_1(\sigma)
  = \sum _{z \in Z}
  \pi^{\sigma_1}[s_1(z)] \cdot
  \pi^{\sigma_2}[s_2(z)] \cdot
  \pi_c(z) \cdot
  u_1(z)
  = \sum _{z \in Z}
  x_{s_1(z)}
  y_{s_2(z)}
  \pi_c(z) \,
  u_1(z)
  = x ^\top A y
\]
So a~best-response realization plan $x$ against arbitrary realization plan~$y$ maximizes the utility $u_1(\sigma) = x ^\top (Ay)$.
By~(\ref{eq:seq-constraints}), we can find such~$x$ as a~solution to the following \acrshort{lp} (parametrized by~$y$):
\begin{equation}
  \label{lp:seq-primal-param}
  \begin{split}
    \max\ (Ay)^\top &x \\
    E&x = e \\
    &x \ge \vect{0}.
  \end{split}
\end{equation}
This \acrshort{lp} has a~finite optimal value, because it is
\begin{enumerate}[(a)]
  \item \emph{feasible}: Any valid realization plan~$x$ will do.
  \item \emph{bounded}: Since $x$ expresses probabilities, $\vect{0} \le x \le \vect{1} = (1, \dots, 1)^\top$, giving an~upper bound $(Ay)^\top x \le \lv Ay \rv ^\top \vect{1}$.
\end{enumerate}
The corresponding dual \acrshort{lp} with a~vector of~dual variables $u \in \mathbb{R}^{1 + \lv \mathcal{I}_1 \rv}$:
\begin{equation}
  \label{lp:seq-dual-param}
  \begin{split}
    \min\  e^\top &u \\
    E^\top &u \ge Ay.
  \end{split}
\end{equation}
is therefore also feasible, bounded and has the equal optimal value by the strong duality theorem.
In a~zero-sum game, when player~$1$ wants to maximize his utility $x^\top Ay = u^\top e$, his opponent wants to minimize it by his choice of~a~realization plan~$y$.
Realization plans are characterized by constraints of~(\ref{eq:seq-constraints}).
All of~this leads to this \acrshort{lp} formulation
\begin{equation}
  \label{lp:seq-dual}
  \begin{split}
    \min_{u, y}\  e^\top u &\\
    E^\top u - Ay &\ge \vect{0} \\
    Fy &= f \\
    y &\ge \vect{0}
  \end{split}
\end{equation}
with such a~dual LP
\begin{equation}
  \label{lp:seq-form}
  \begin{split}
    \max_{v, x}\  f^\top v & \\
    Ex &= e \\
    F^\top v - A^\top x &\le \vect{0} \\
    x &\ge \vect{0},
  \end{split}
\end{equation}
where $v$ is the vector of~(negative) \acrshort{cbv}s for player~$2$ (\cite{Cermak2014practical, AGT07}).
The (\ref{lp:seq-form}) is the standard \emph{sequence-form \acrshort{lp} formulation} used for solving \acrshort{efg}s.

Notably, $E$, $F$ and $A$ are sparse matrices, with the number of~variables and non-zero values linear in~the size of~the game tree.
So zero-sum \acrshort{efg}s can be solved by (dual) linear programming for the \emph{linear-sized} \acrshort{lp}s~(\ref{lp:seq-form}).

\section{Solving Games with Learning Algorithms}
\epigraphLong{
  Perfecting oneself is as~much unlearning as~it is learning.
}{Edsger Dijkstra}
In~contrast to solving games by linear programming, we can approximate \acrlong{ne} of~\acrshort{efg}s using various \emph{learning algorithms}:
\begin{description}
  \item [(Vanilla) \acrfull{cfr}] is a~\emph{self-playing} algorithm based on~\emph{regret matching}.
    In the learning phase, it improves itself by summing the total amount of~\emph{counterfactual regret} (a~regret related to counterfactual values), for every action at~each decision point.
    The average strategy over all iterations converges towards a~\acrshort{ne} for the game, and it is thus used as the final strategy during the play.
    (\cite{Zinkevich2007regret})

  \item [\acrfull{mccfr}] is a~\emph{Monte Carlo} version of~\acrshort{cfr}.
    Rather than traversing the full game tree, it probabilistically samples actions to speed up computation per learning iteration.
    The partial tree exploration is nevertheless compensated by weighting regrets with appropriate probabilities.
    (\cite{Johanson2012efficient})

  \item [\cfrplus] is a~new \acrshort{cfr}-type algorithm based on~\emph{regret-matching}\textsuperscript{+}.
    A~\emph{cumulative counterfactual regret}\textsuperscript{+} ($\max(\cdot, 0)$) is used instead.
    It typically outperforms the previously known algorithms by an~order of~magnitude or more in terms of~computation time, while also potentially requiring less memory.
    Another advantage is that many of the cumulative regret values are zero, whereas in~\acrshort{cfr}, negative regret continues to accumulate indefinitely.
    This reduces the entropy of~the data needed during the computation. 
    (\cite{Tammelin2015solving})
\end{description}
These iterative methods run faster than methods with sequence-form \acrshort{lp}s, and use significantly less memory.
Moreover, they guarantee a~convergence to \acrlong{ne} in~limit (\cite[Theorem~4]{Zinkevich2007regret}).
Therefore, they currently prevail as dominant solutions, and are standardly used for solving large imperfect-information games.
For a~detailed overview, consult also (\cite{Bosansky2013solving}).

\section{Subgames Revisited}
\label{sec:subgames-revisited}
\epigraphLong{
  When you think you can't, revisit a previous triumph.
}{Jack Canfield}
So far, the information sets have grouped only those states where the player was the acting player.
For the purposes of the subgame, it is also necessary to include the states that are indistinguishable from the point of view of other players.
Therefore, we define the \emph{augmented information sets} (\cite{BurchJohansonBowling2014}):
\begin{framed}
  \begin{defn}[augmented information set]
    \label{defn:augm-info-set}
    For any player $i \in P$, let $H_i(h)$ be the sequence of player $i$'s information sets reached by player $i$ on the path to $h$, and the actions taken by player~$i$.

    Then \emph{augmented information set} $I_i(h)$ is defined by the following characterization:
    \[ I_i (h) = I_i (h') \; \Longleftrightarrow \; H_i (h) = H_i (h') \]
    for any two states $h, h' \in H$.
  \end{defn}
\end{framed}

At this point, we may finally define the notion of a~\emph{subgame} (\cite{BurchJohansonBowling2014}):
\begin{framed}
  \begin{defn}[subgame]
    \label{defn:subgame}
    An~(imperfect-information) \textbf{subgame} is a~forest of~subtrees, closed under both the descendant relation and membership within augmented information sets for any player.
  \end{defn}
\end{framed}
Consult Section~\ref{sec:naive-rps-subgame} for an~illustrative example.
