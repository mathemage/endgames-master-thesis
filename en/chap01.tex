\chapter{Background, Definitions and Notations}

The field of Game Theory deals with interactions or conflicts between $2$ or more agents.
\todo

\section{General Game Theory}

\begin{itemize}
  \item players
  \item actions
  \item payoffs
  \item \emph{Nash equilibrium}
  \item \emph{best response}
\end{itemize}

\subsection{Extensive form}

An \emph{extensive-form game} consists of

\begin{itemize}
  \item a finite set of \emph{players} $P$,
  \item a finite set $H$ of all possible \emph{histories},
    \begin{itemize}
      \item Each history consists of individual \emph{actions}.
      \item $h \sqsubseteq h'$ denotes that history~$h$ is a~prefix of $h'$.
      \item $\emptyset \in H$ and $h' \in H \land h \sqsubseteq h' \implies h \in H$
      \item Set $Z \subseteq H$ is the set of \emph{terminal histories}, i.e. histories that are not prefixes of any other histories.
    \end{itemize}
  \item the set of available actions $A(h) = \braces{a: (h, a) \in H}$ for every node $h \in H \setminus Z$,
  \item a~function $p$ assigning an~\emph{acting player} to each $h \in H \setminus Z$.
    The acting players are taken from the set $P \cup \braces{c}$, where $c$ is the \textbf{c}hance player (e.~g. a~dice, the card dealer, the nature etc.).
  \item a~function $f_c$ determining the probability measure over actions $A(h)$ for nodes $h$ with $p(h) = c$, the nodes of the chance player.
  \item The partition $\I_i$ of nodes $\braces{h \in H: p(h) = i}$ is called the \emph{information partition} of player~$i$.
    Its element $I \in \I_i$ is an \emph{information set} of player~$i$.
    An information set represents grouping of histories that are indistinguishable from $i$'s point of view.
    In the game of poker, for example, this might be because of hidden cards of opponents.
  \item a \emph{utility function} $u_i\colon Z \goto \R$,
\end{itemize}

There are further notions related to any extensive-form game:

\begin{itemize}
  \item \emph{Strategy}~$\sigma_i$ of (non-chance) player~$i$ determines a~probability distribution over $A(I)$ at every $I \in \I_i$.
    Thus $\pi ^\sigma (I, a)$ is the probability of action $a$ at the information set~$I$.
    $\Sigma_i$ denotes the set of all possible strategies for player~$i$.
  \item A~\emph{strategy profile} is a~vector of all (non-chance) players' strategies denoted by $\sigma = (\sigma_1, \sigma_2, \ldots, \sigma_ {\abs{P}})$.
    The set of all such possible strategy profiles is denoted by $\Sigma$.
    Hence, it is the Cartesian product $\Sigma = \prod\limits _{i \in P} \Sigma_i$.
  \item We use the Greek letter $\pi$ to evaluate the probability corresponding to a~profile~$\sigma$:
    \[\pi ^\sigma(h) = \prod _{i \in P \cup \braces{c}} \pi _i ^\sigma (h)\]
    
  \item The probability $\pi _{-i} ^\sigma (h)$ (or sometimes just briefly $\pi _{-i} (h)$) is the product of~all players' contribution, except for the one of player~$i$:
    \[\pi _{-i} ^\sigma(h) = \prod _{j \in P \setminus \braces{i} \cup \braces{c}} \pi _j ^\sigma (h)\]
    
  \item $\sigma | _{I \goto a}$ denotes the strategy identical to $\sigma$ with the only one exception:
    the action~$a$ is always played at the information set~$I$.
  \item Given the strategic profile $\sigma$, the \emph{expected utility}~$u_i (\sigma)$ for player~$i$ is defined as:
    \[ u_i (\sigma) = \sum _{z \in Z} \pi ^\sigma (z) u_i(z)\]

  \item A~\emph{best response} $BR _i (\sigma)$ of player $i$ for given $\sigma _{-p}$ is such a~strategy $\sigma _i \in \Sigma _i$ that maximizes player's expected utility against others:
    \[ u_i (\sigma) = \max _{\sigma'_i \in \Sigma_i} u_i ((\sigma'_i, \sigma_{-i})) \]

  \item A~\emph{Nash equilibrium} (in the context of extensive-form games) is a~strategy profile $\sigma$ such that no player~$i \in P$ has any incentive to deviate from his strategy.
    In other words, all players are playing best responses against each other:
    \[ \forall i \in P\colon u_i (\sigma) = \max _{\sigma'_i \in \Sigma_i} u_i ((\sigma'_i, \sigma_{-i})) \]

  \item The \emph{counterfactual value} $v _i ^\sigma (I)$ is the expected utility provided that the information set $I$ is reached and all players play according to strategy $\sigma$ with exception of player~$i$, who plays to reach $I$:
    \[ v _i ^\sigma (I) = \sum\limits _{h \in I, \; h' \in Z}
      \frac
      {\pi _{-i} ^\sigma(h) \pi ^\sigma(h,h') u_i(h')}
      {\pi _{-i} ^\sigma (I)} \]

  \item A~\emph{counterfactual best response} $CBR _i (\sigma)$ of player~$i$ is a~strategy maximizing the counterfactual value at each information set $I \in \I _i$:
    \[ \pi ^\sigma (I, a) \geq 0
      \; \Longleftrightarrow \;
      v _i ^\sigma (I, a) = \max _{a' \in A(I)} v _i ^\sigma (I, a') \]

    Note that $CBR _i (\sigma)$ is always a best response $BR _i (\sigma)$, but the reverse implication does not need to hold:
    a~best response $\sigma$ can select an~arbitrary action in an~unreachable information set $I$ (the one where $\pi ^\sigma (I) = 0$).
    Such best responses are not in general counterfactual best responses.

  \item \emph{counterfactual best response value}
\end{itemize}

There may be various properties, which an~extensive-form game can have.
Such a~game can:

\begin{itemize}
  \item be \emph{two-player}
  \item have \emph{perfect recall}
  \item be \emph{zero-sum}
\end{itemize}

\subsection{Sequence form}

\section{Methods of~solution}

\subsection{Linear programming}

\subsection{Learning}

\section{Subgames (endgames)}

\subsection{Previous works}
